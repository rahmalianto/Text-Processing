{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----stemmedWords-----\n",
      "[['charact', 'segment', 'handwritten', 'manuscript', 'often', 'present', 'complic', 'task', 'mani', 'factor', 'caus', 'segment', 'difficult', 'inconsist', 'slope', 'slant', 'length', 'width', 'charact', 'well', 'intersect', 'two', 'charact', 'either', 'differ', 'line', 'paper', 'propos', 'new', 'approach', 'combin', 'statist', 'structur', 'analys', 'gener', 'javanes', 'script', 'line', 'segment', 'javanes', 'manuscript', 'imag', 'everi', 'time', 'new', 'manuscript', 'discov', 'object', 'make', 'charact', 'manuscript', 'identifi', 'use', 'interconnect', 'oper', 'identifi', 'compon', 'script', 'object', 'interconnect', 'given', 'label', 'next', 'task', 'calcul', 'averag', 'height', 'averag', 'width', 'object', 'given', 'label', 'standard', 'deviat', 'inform', 'use', 'guid', 'averag', 'normal', 'script', 'ie', 'charact', 'height', 'width', 'exce', 'averag', 'valu', 'plu', 'standard', 'deviat', 'conclud', 'charact', 'question', 'fact', 'consist', 'two', 'charact', 'touch', 'regard', 'normal', 'skew', 'cluster', 'script', 'task', 'straighten', 'script', 'way', 'becom', 'perpendicular', 'experi', 'done', 'use', '13', 'line', 'imag', 'differ', 'author', 'differ', 'write', 'style', 'result', 'show', '8819', 'segment', 'accuraci', 'conclud', 'propos', 'approach', 'segment', 'method', 'rel', 'success', 'appli', 'javanes', 'handwritten', 'charact'], ['music', 'close', 'relat', 'human', 'psycholog', 'piec', 'music', 'often', 'associ', 'certain', 'adject', 'happi', 'sad', 'romant', 'linkag', 'music', 'certain', 'mood', 'wide', 'use', 'variou', 'occas', 'peopl', 'music', 'classif', 'base', 'relev', 'particular', 'emot', 'import', 'research', 'concern', 'music', 'classif', 'system', 'base', 'mood', 'paramet', 'use', 'k-nearest', 'neighbor', 'classif', 'method', 'self', 'organ', 'map', 'mood', 'paramet', 'use', 'base', 'robert', 'thayer', \"'s\", 'energy-stress', 'model', 'exuber', '/', 'happi', 'content', '/', 'relax', 'anxiou', 'depress', 'featur', 'use', 'rhythm', 'pattern', 'music', 'system', 'built', 'addit', 'facil', 'play', 'song', 'accord', 'mood', 'chosen', 'system', 'test', 'use', 'set', 'kid', 'song', 'show', 'number', 'cluster', 'mood', 'song', 'collect', 'classif', 'result', 'obtain', 'two', 'classif', 'method', 'k-nearest', 'neighbor', 'self', 'organ', 'map', 'compar', 'mood', 'obtain', 'child', 'psycholog', 'expert'], ['world', 'wide', 'web', 'evolv', 'drastic', 'recent', 'advent', 'social', 'network', 'media', 'rich', 'websit', 'necessit', 'analysi', 'social', 'network', 'opinion', 'express', 'variou', 'user', 'media', 'like', 'blog', 'tweet', 'websit', 'page', 'alik', 'lot', 'previou', 'research', 'done', 'product', '/', 'crm', 'domain', 'rel', 'research', 'done', 'network', 'analysi', 'social', 'media', 'focu', 'extract', 'opinion', 'e-health', 'data', 'ie', 'data', 'express', 'variou', 'blog', 'websit', 'page', 'user', 'regard', 'variou', 'aspect', 'health', 'market', 'medic', 'compani', 'leverag', 'data', 'augment', 'custom', 'reach', 'thu', 'profit', 'sentiment', 'analysi', 'network', 'analysi', 'data', 'help', 'variou', 'organ', 'understand', 'health', 'pattern', 'address', 'peopl', \"'s\", 'concern', 'predict', 'outbreak', 'pattern', 'case', 'contagi', 'diseas', 'paper', 'outlin', 'machin', 'techniqu', 'help', 'analysi', 'medic', 'domain', 'data', 'social', 'network', 'paper', 'compar', 'exist', 'techniqu', 'machin', 'learn', 'discuss', 'advantag', 'challeng', 'encompass', 'perspect', 'involv', 'use', 'text', 'mine', 'method', 'applic', 'e-health', 'madicin', 'evalu', 'medic', 'domain', 'data', 'classif', 'effici', 'use', 'variou', 'metric', 'like', 'roc', 'auc', 'implement', 'via', 'r', 'languag', 'packag', 'need', 'infer', 'machin', 'learn', 'techniqu', 'relev', 'process', 'data', 'social', 'network', 'medic', 'term', 'name', 'deni', 'btw'], ['sentiment', 'analysi', 'comput', 'studi', 'opinion', 'sentiment', 'evalu', 'attitud', 'view', 'emot', 'express', 'text', 'refer', 'classif', 'problem', 'main', 'focu', 'predict', 'polar', 'word', 'classifi', 'posit', 'neg', 'sentiment', 'sentiment', 'analysi', 'twitter', 'offer', 'peopl', 'fast', 'effect', 'way', 'measur', 'public', '’', 'feel', 'toward', 'parti', 'politician', 'primari', 'issu', 'previou', 'sentiment', 'analysi', 'techniqu', 'determin', 'appropri', 'classifi', 'given', 'classif', 'problem', 'one', 'classifi', 'chosen', 'avail', 'classifi', 'sureti', 'best', 'perform', 'unseen', 'data', 'reduc', 'risk', 'select', 'inappropri', 'classifi', 'combin', 'output', 'set', 'classifi', 'thu', 'paper', 'use', 'approach', 'automat', 'classifi', 'sentiment', 'tweet', 'combin', 'machin', 'learn', 'classifi', 'lexicon', 'base', 'classifi', 'new', 'combin', 'classifi', 'sentiwordnet', 'classifi', 'naiv', 'bay', 'classifi', 'hidden', 'markov', 'model', 'classifi', 'posit', 'neg', 'tweet', 'determin', 'use', 'major', 'vote', 'principl', 'result', 'three', 'classifi', 'thu', 'use', 'sentiment', 'classifi', 'find', 'polit', 'sentiment', 'real', 'time', 'tweet', 'thu', 'got', 'improv', 'accuraci', 'sentiment', 'analysi', 'use', 'classifi', 'ensembl', 'approach', 'method', 'also', 'use', 'negat', 'handl', 'word', 'sens', 'disambigu', 'achiev', 'high', 'accuraci']]\n",
      "-----train set-----\n",
      "['charact segment handwritten manuscript often present complic task mani factor caus segment difficult inconsist slope slant length width charact well intersect two charact either differ line paper propos new approach combin statist structur analys gener javanes script line segment javanes manuscript imag everi time new manuscript discov object make charact manuscript identifi use interconnect oper identifi compon script object interconnect given label next task calcul averag height averag width object given label standard deviat inform use guid averag normal script ie charact height width exce averag valu plu standard deviat conclud charact question fact consist two charact touch regard normal skew cluster script task straighten script way becom perpendicular experi done use 13 line imag differ author differ write style result show 8819 segment accuraci conclud propos approach segment method rel success appli javanes handwritten charact', \"music close relat human psycholog piec music often associ certain adject happi sad romant linkag music certain mood wide use variou occas peopl music classif base relev particular emot import research concern music classif system base mood paramet use k-nearest neighbor classif method self organ map mood paramet use base robert thayer 's energy-stress model exuber / happi content / relax anxiou depress featur use rhythm pattern music system built addit facil play song accord mood chosen system test use set kid song show number cluster mood song collect classif result obtain two classif method k-nearest neighbor self organ map compar mood obtain child psycholog expert\", \"world wide web evolv drastic recent advent social network media rich websit necessit analysi social network opinion express variou user media like blog tweet websit page alik lot previou research done product / crm domain rel research done network analysi social media focu extract opinion e-health data ie data express variou blog websit page user regard variou aspect health market medic compani leverag data augment custom reach thu profit sentiment analysi network analysi data help variou organ understand health pattern address peopl 's concern predict outbreak pattern case contagi diseas paper outlin machin techniqu help analysi medic domain data social network paper compar exist techniqu machin learn discuss advantag challeng encompass perspect involv use text mine method applic e-health madicin evalu medic domain data classif effici use variou metric like roc auc implement via r languag packag need infer machin learn techniqu relev process data social network medic term name deni btw\", 'sentiment analysi comput studi opinion sentiment evalu attitud view emot express text refer classif problem main focu predict polar word classifi posit neg sentiment sentiment analysi twitter offer peopl fast effect way measur public ’ feel toward parti politician primari issu previou sentiment analysi techniqu determin appropri classifi given classif problem one classifi chosen avail classifi sureti best perform unseen data reduc risk select inappropri classifi combin output set classifi thu paper use approach automat classifi sentiment tweet combin machin learn classifi lexicon base classifi new combin classifi sentiwordnet classifi naiv bay classifi hidden markov model classifi posit neg tweet determin use major vote principl result three classifi thu use sentiment classifi find polit sentiment real time tweet thu got improv accuraci sentiment analysi use classifi ensembl approach method also use negat handl word sens disambigu achiev high accuraci']\n",
      "-----Term FREQUENCY-----\n",
      "[[0 0 1 ... 0 1 0]\n",
      " [1 2 0 ... 0 0 0]\n",
      " [1 1 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 1]]\n",
      "-----VECTOR FITUR-----\n",
      "[\"'s\", '/', '13', '8819', 'accord', 'accuraci', 'achiev', 'addit', 'address', 'adject', 'advantag', 'advent', 'alik', 'also', 'analys', 'analysi', 'anxiou', 'appli', 'applic', 'approach', 'appropri', 'aspect', 'associ', 'attitud', 'auc', 'augment', 'author', 'automat', 'avail', 'averag', 'base', 'bay', 'becom', 'best', 'blog', 'btw', 'built', 'calcul', 'case', 'caus', 'certain', 'challeng', 'charact', 'child', 'chosen', 'classif', 'classifi', 'close', 'cluster', 'collect', 'combin', 'compani', 'compar', 'complic', 'compon', 'comput', 'concern', 'conclud', 'consist', 'contagi', 'content', 'crm', 'custom', 'data', 'deni', 'depress', 'determin', 'deviat', 'differ', 'difficult', 'disambigu', 'discov', 'discuss', 'diseas', 'domain', 'done', 'drastic', 'e-health', 'effect', 'effici', 'either', 'emot', 'encompass', 'energy-stress', 'ensembl', 'evalu', 'everi', 'evolv', 'exce', 'exist', 'experi', 'expert', 'express', 'extract', 'exuber', 'facil', 'fact', 'factor', 'fast', 'featur', 'feel', 'find', 'focu', 'gener', 'given', 'got', 'guid', 'handl', 'handwritten', 'happi', 'health', 'height', 'help', 'hidden', 'high', 'human', 'identifi', 'ie', 'imag', 'implement', 'import', 'improv', 'inappropri', 'inconsist', 'infer', 'inform', 'interconnect', 'intersect', 'involv', 'issu', 'javanes', 'k-nearest', 'kid', 'label', 'languag', 'learn', 'length', 'leverag', 'lexicon', 'like', 'line', 'linkag', 'lot', 'machin', 'madicin', 'main', 'major', 'make', 'mani', 'manuscript', 'map', 'market', 'markov', 'measur', 'media', 'medic', 'method', 'metric', 'mine', 'model', 'mood', 'music', 'naiv', 'name', 'necessit', 'need', 'neg', 'negat', 'neighbor', 'network', 'new', 'next', 'normal', 'number', 'object', 'obtain', 'occas', 'offer', 'often', 'one', 'oper', 'opinion', 'organ', 'outbreak', 'outlin', 'output', 'packag', 'page', 'paper', 'paramet', 'parti', 'particular', 'pattern', 'peopl', 'perform', 'perpendicular', 'perspect', 'piec', 'play', 'plu', 'polar', 'polit', 'politician', 'posit', 'predict', 'present', 'previou', 'primari', 'principl', 'problem', 'process', 'product', 'profit', 'propos', 'psycholog', 'public', 'question', 'r', 'reach', 'real', 'recent', 'reduc', 'refer', 'regard', 'rel', 'relat', 'relax', 'relev', 'research', 'result', 'rhythm', 'rich', 'risk', 'robert', 'roc', 'romant', 'sad', 'script', 'segment', 'select', 'self', 'sens', 'sentiment', 'sentiwordnet', 'set', 'show', 'skew', 'slant', 'slope', 'social', 'song', 'standard', 'statist', 'straighten', 'structur', 'studi', 'style', 'success', 'sureti', 'system', 'task', 'techniqu', 'term', 'test', 'text', 'thayer', 'three', 'thu', 'time', 'touch', 'toward', 'tweet', 'twitter', 'two', 'understand', 'unseen', 'use', 'user', 'valu', 'variou', 'via', 'view', 'vote', 'way', 'web', 'websit', 'well', 'wide', 'width', 'word', 'world', 'write', '’']\n",
      "-----NUMBER VECTOR FEATURES-----\n",
      "293\n",
      "-----dfs-----\n",
      "[2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 4, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1]\n",
      "-----query----\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'!' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-eb2ee0eba576>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataQuery\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[1;31m# calculate tfidf with formula tfidf = tf*log(Ndocument/df)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m     \u001b[0mqueryTfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mdfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabQuery\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m     \u001b[0mquery_data_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueryTfidf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[0mxls\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueryTfidf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\",\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: '!' is not in list"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "##import glob\n",
    "##import errno\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "##path = 'documents/download/*.txt'\n",
    "##files = glob.glob(path)\n",
    "##\n",
    "##for name in files:\n",
    "##    try:\n",
    "##        with open(name) as f:\n",
    "##            print(name)\n",
    "##    except IOError as exc:\n",
    "##        if exc.errno != errno.EISDIR:\n",
    "##            raise\n",
    "\n",
    "REGEX = re.compile(r\"\\s\")\n",
    "def tokenize(text):\n",
    "    return [tok.strip().lower() for tok in REGEX.split(text)]\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# open all passages\n",
    "text = []\n",
    "file = open(\"documents/text1.txt\",\"r\");\n",
    "text1 = file.read()\n",
    "text.append(text1)\n",
    "file = open(\"documents/text2.txt\",\"r\");\n",
    "text2 = file.read()\n",
    "text.append(text2)\n",
    "file = open(\"documents/text3.txt\",\"r\");\n",
    "text3 = file.read()\n",
    "text.append(text3)\n",
    "file = open(\"documents/text4.txt\",\"r\");\n",
    "text4 = file.read()\n",
    "text.append(text4)\n",
    "##print(text)\n",
    "##print(len(text))\n",
    "\n",
    "\n",
    "# dibuat huruf kecil semua\n",
    "text = [s.lower() for s in text]\n",
    "##print(text)\n",
    "\n",
    "\n",
    "# remove all punctuations\n",
    "punctuation = [\".\" , \",\" , \"(\" , \")\" , \"%\"]\n",
    "i = 0\n",
    "for row in text:\n",
    "    text[i] = ''.join([p for p in text[i] if p not in punctuation])\n",
    "    i += 1\n",
    "##print(\"-----text-----\")\n",
    "##print(text)\n",
    "\n",
    "\n",
    "# tokenize the article into words\n",
    "i = 0\n",
    "words = [];\n",
    "for row in text:\n",
    "    words.append(word_tokenize(text[i]))\n",
    "    i += 1\n",
    "##print(\"-----words-----\")\n",
    "##print(words)\n",
    "\n",
    "                             \n",
    "# filter the stopwords\n",
    "i = 0\n",
    "filteredText = [];\n",
    "for row in words:\n",
    "    filteredText.append([w for w in words[i] if not w in stop_words])\n",
    "    i += 1\n",
    "##print(\"-----filtered-----\")\n",
    "##print(filteredText)\n",
    "\n",
    "\n",
    "# stem every word\n",
    "i = 0\n",
    "stemmedWords = [];\n",
    "for row in filteredText:\n",
    "    stemmedWords.append([ps.stem(w) for w in filteredText[i]])\n",
    "    i += 1\n",
    "print(\"-----stemmedWords-----\")\n",
    "print(stemmedWords)\n",
    "\n",
    "\n",
    "train_set = [' '.join(w) for w in stemmedWords]\n",
    "print(\"-----train set-----\")\n",
    "print(train_set)\n",
    "\n",
    "\n",
    "count_vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "data = count_vectorizer.fit_transform(train_set).toarray()\n",
    "vocab = count_vectorizer.get_feature_names()\n",
    "\n",
    "print (\"-----Term FREQUENCY-----\")\n",
    "print (data)\n",
    "print (\"-----VECTOR FITUR-----\")\n",
    "print (vocab)\n",
    "print (\"-----NUMBER VECTOR FEATURES-----\")\n",
    "print (len(vocab))\n",
    "\n",
    "\n",
    "# export excel terms Frequency\n",
    "xls=\"Term-Frequency,\"\n",
    "for vc in vocab:\n",
    "\txls+=vc+\",\"\n",
    "\n",
    "xls += \"\\n\"\n",
    "\n",
    "# print terms Frequency for each document\n",
    "i = 0\n",
    "for row in data:\n",
    "\txls += \"Document \"+str(i)+\",\"\n",
    "\tfor col in row:\n",
    "\t\txls += str(col)+\",\"\n",
    "\txls += \"\\n\"\t\n",
    "\ti += 1\n",
    "\n",
    "\n",
    "\n",
    "# export excel Document Frequency\n",
    "xls += \"\\n\\n\\n\"\n",
    "xls += \"Document-Frequency,\"\n",
    "for vc in vocab:\n",
    "\txls+=vc+\",\"\n",
    "\n",
    "xls += \"\\n\"\n",
    "\n",
    "dfs = []\n",
    "for row in data:\n",
    "\ti = 0\n",
    "\txls += \",\"\n",
    "\tfor col in row:\n",
    "\t\tcount = 0\n",
    "\t\tif data[0][i]>0:count += 1\n",
    "\t\tif data[1][i]>0:count += 1\n",
    "\t\tif data[2][i]>0:count += 1\n",
    "\t\tif data[3][i]>0:count += 1\n",
    "\t\tdfs.append(count)\n",
    "\t\txls += str(count)+\",\"\n",
    "\t\ti += 1\n",
    "\txls += \"\\n\"\n",
    "\tbreak\n",
    "\n",
    "print(\"-----dfs-----\")\n",
    "print(dfs)\n",
    "\n",
    "### sort document Frequency\n",
    "##xls += \"\\n\\n\\n\"\n",
    "##xls += \"Document-Frequency,\"\n",
    "\n",
    "# export TF-IDF\n",
    "xls += \"\\n\\n\\n\"\n",
    "xls += \"TF-IDF,\"\n",
    "for vc in vocab:\n",
    "\txls+=vc+\",\"\n",
    "\n",
    "xls += \"\\n\"\n",
    "\n",
    "i=0\n",
    "TFIDF=[]\n",
    "for row in data:\n",
    "\txls += \"DOCUMENT \"+str(i)+\",\"\n",
    "\tj=0\n",
    "\tdata_tfidf=[]\n",
    "\tfor col in row:\n",
    "\t\t# calculate tfidf with formula tfidf = tf*log(Ndocument/df)\n",
    "\t\ttfidf = col*math.log(4/dfs[j])\n",
    "\t\tdata_tfidf.append(tfidf)\n",
    "\t\txls += str(tfidf)+\",\"\n",
    "\t\tj += 1\n",
    "\t\n",
    "\tTFIDF.append(data_tfidf)\n",
    "\txls += \"\\n\"\n",
    "\ti += 1\n",
    "\n",
    "xls += \"\\n\\n\\n\"\n",
    "xls+= \",\"\n",
    "for i in range(4):\n",
    "\txls += \"Document\"+str(i+1)+\",\"\n",
    "xls += \"\\n\"\n",
    "\n",
    "for i in range(4):\n",
    "\txls += \"Document\"+str(i+1)+\",\"\n",
    "\tfor j in range(4):\n",
    "\t\tdt1 = TFIDF[i]\n",
    "\t\tdt2 = TFIDF[j]\n",
    "\t\t# calculate dot product\n",
    "\t\tdotproduct = np.dot(dt1,dt2)\n",
    "\t\t# calculate 2 data magnitude\n",
    "\t\tmagnitude1 = np.sqrt(np.dot(dt1,dt1))\n",
    "\t\tmagnitude2 = np.sqrt(np.dot(dt2,dt2))\n",
    "\n",
    "\t\tcos_similarity = dotproduct/(magnitude1*magnitude2)\n",
    "\t\txls += str(cos_similarity)+\",\"\n",
    "\txls += \"\\n\"\n",
    "\n",
    "\n",
    "##file = open(\"tf-idf.csv\",\"w\")\n",
    "##file.write(xls)\n",
    "##print (\"file created\")\n",
    "\n",
    "##new query\n",
    "print(\"-----query----\")\n",
    "file = open(\"documents/follower.txt\",\"r\");\n",
    "query = file.read()\n",
    "query = query.lower()\n",
    "query = ''.join([p for p in query if p not in punctuation])\n",
    "tokenizedQuery = word_tokenize(query)\n",
    "filteredQuery = [w for w in tokenizedQuery if not w in stop_words]\n",
    "stemmedQuery = [ps.stem(w) for w in filteredQuery]\n",
    "joinedQuery = [' '.join(stemmedQuery)]\n",
    "\n",
    "dataQuery = count_vectorizer.fit_transform(joinedQuery).toarray()\n",
    "vocabQuery = count_vectorizer.get_feature_names()\n",
    "\n",
    "# print terms Frequency for query\n",
    "xls += \"\\n\\n\\n\"\n",
    "xls += \"Query Term-Frequency,\"\n",
    "for vc in vocabQuery:\n",
    "\txls+=vc+\",\"\n",
    "\n",
    "xls += \"\\n\"\n",
    "for row in dataQuery:\n",
    "\txls += \"Query \" + \",\"\n",
    "\t\n",
    "\tfor col in row:\n",
    "\t\txls += str(col)+\",\"\n",
    "\t\t\n",
    "# export TF-IDF\n",
    "xls += \"\\n\\n\\n\"\n",
    "xls += \"Query TF-IDF,\"\n",
    "for vc in vocabQuery:\n",
    "\txls+=vc+\",\"\n",
    "\n",
    "xls += \"\\n\"\n",
    "\n",
    "QueryTFIDF=[0]*len(TFIDF[0])\n",
    "xls += \"Query \"+\",\"\n",
    "j=0\n",
    "query_data_tfidf=[]\n",
    "queryTfidf = 0;\n",
    "for col in dataQuery[0]:\n",
    "    # calculate tfidf with formula tfidf = tf*log(Ndocument/df)\n",
    "    queryTfidf = col*math.log(4/dfs[vocab.index(vocabQuery[j])])\n",
    "    query_data_tfidf.append(queryTfidf)\n",
    "    xls += str(queryTfidf) + \",\"\n",
    "\n",
    "    j += 1\n",
    "\n",
    "#find index in vocab\n",
    "i=0\n",
    "for col in vocabQuery:\n",
    "    QueryTFIDF[vocab.index(col)] = query_data_tfidf[i]\n",
    "    i += 1\n",
    "    \n",
    "xls += \"\\n\\n\\n\"\n",
    "i=0\n",
    "xls+= \",\"\n",
    "for i in range(4):\n",
    "\txls += \"Document\"+str(i+1)+\",\"\n",
    "xls += \"\\n\"\n",
    "\n",
    "xls += \"Query\"+\",\"\n",
    "cos_similarity=[]\n",
    "i=0\n",
    "for j in range(4):\n",
    "    dt1 = QueryTFIDF\n",
    "    dt2 = TFIDF[j]\n",
    "    # calculate dot product\n",
    "    dotproduct = np.dot(dt1,dt2)\n",
    "    # calculate 2 data magnitude\n",
    "    magnitude1 = np.sqrt(np.dot(dt1,dt1))\n",
    "    magnitude2 = np.sqrt(np.dot(dt2,dt2))\n",
    "\n",
    "    cos_similarity.append(dotproduct/(magnitude1*magnitude2))\n",
    "    xls += str(cos_similarity[i])+\",\"\n",
    "    i += 1\n",
    "\n",
    "\t\n",
    "file = open(\"tf-idf.csv\",\"w\")\n",
    "file.write(xls)\n",
    "print (\"file created\")\n",
    "print (\"Query is similar to document:\", cos_similarity.index(max(cos_similarity))+1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
