{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['charact segment handwritten manuscript often present complic task mani factor caus segment difficult inconsist slope slant length width charact well intersect two charact either differ line paper propos new approach combin statist structur analys gener javanes script line segment javanes manuscript imag everi time new manuscript discov object make charact manuscript identifi use interconnect oper identifi compon script object interconnect given label next task calcul averag height averag width object given label standard deviat inform use guid averag normal script ie charact height width exce averag valu plu standard deviat conclud charact question fact consist two charact touch regard normal skew cluster script task straighten script way becom perpendicular experi done use 13 line imag differ author differ write style result show 8819 segment accuraci conclud propos approach segment method rel success appli javanes handwritten charact', 'charact segment handwritten manuscript often present complic task mani factor caus segment difficult inconsist slope slant length width charact well intersect two charact either differ line paper propos new approach combin statist structur analys gener javanes script line segment javanes manuscript imag everi time new manuscript discov object make charact manuscript identifi use interconnect oper identifi compon script object interconnect given label next task calcul averag height averag width object given label standard deviat inform use guid averag normal script ie charact height width exce averag valu plu standard deviat conclud charact question fact consist two charact touch regard normal skew cluster script task straighten script way becom perpendicular experi done use 13 line imag differ author differ write style result show 8819 segment accuraci conclud propos approach segment method rel success appli javanes handwritten charact', \"world wide web evolv drastic recent advent social network media rich websit necessit analysi social network opinion express variou user media like blog tweet websit page alik lot previou research done product / crm domain rel research done network analysi social media focu extract opinion e-health data ie data express variou blog websit page user regard variou aspect health market medic compani leverag data augment custom reach thu profit sentiment analysi network analysi data help variou organ understand health pattern address peopl 's concern predict outbreak pattern case contagi diseas paper outlin machin learn techniqu help analysi medic domain data social network paper compar exist techniqu machin learn discuss advantag challeng encompass perspect involv use text mine method applic e-health madicin evalu medic domain data classif effici use variou metric like roc auc implement via r languag packag need infer machin learn techniqu relev process data social network medic term\", 'sentiment analysi comput studi opinion sentiment evalu attitud view emot express text refer classif problem main focu predict polar word classifi posit neg sentiment sentiment analysi twitter offer peopl fast effect way measur public ’ feel toward parti politician primari issu previou sentiment analysi techniqu determin appropri classifi given classif problem one classifi chosen avail classifi sureti best perform unseen data reduc risk select inappropri classifi combin output set classifi thu paper use approach automat classifi sentiment tweet combin machin learn classifi lexicon base classifi new combin classifi sentiwordnet classifi naiv bay classifi hidden markov model classifi posit neg tweet determin use major vote principl result three classifi thu use sentiment classifi find polit sentiment real time tweet thu got improv accuraci sentiment analysi use classifi ensembl approach method also use negat handl word sens disambigu achiev high accuraci']\n",
      "Jumlah Term FREQUENCY\n",
      "[[ 0  0  1  1  1  0  0  0  0  0  0  1  0  1  0  2  0  0  0  0  0  1  0  0\n",
      "   4  0  0  1  0  0  1  0  1  0  8  0  0  0  1  1  0  0  1  1  0  0  2  1\n",
      "   0  0  0  0  0  2  3  1  0  1  0  0  0  1  0  0  0  0  1  0  0  0  0  1\n",
      "   0  1  0  1  0  0  1  1  0  0  0  0  1  2  0  1  0  2  0  2  0  0  0  2\n",
      "   1  2  0  0  0  1  0  1  2  1  0  0  3  2  0  0  1  0  0  0  3  0  0  0\n",
      "   0  0  1  1  4  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  2  1  2  3\n",
      "   0  1  0  1  0  0  0  0  0  0  0  1  0  0  0  0  1  0  1  0  0  0  0  0\n",
      "   1  0  0  0  0  0  0  0  2  0  1  0  0  0  0  0  0  1  1  0  0  1  0  0\n",
      "   0  5  5  0  0  0  0  0  1  1  1  1  0  2  1  1  1  0  1  1  0  3  0  0\n",
      "   0  0  0  1  1  0  0  0  2  0  0  3  0  1  0  0  0  0  1  0  0  1  0  3\n",
      "   0  0  1  0]\n",
      " [ 0  0  1  1  1  0  0  0  0  0  0  1  0  1  0  2  0  0  0  0  0  1  0  0\n",
      "   4  0  0  1  0  0  1  0  1  0  8  0  0  0  1  1  0  0  1  1  0  0  2  1\n",
      "   0  0  0  0  0  2  3  1  0  1  0  0  0  1  0  0  0  0  1  0  0  0  0  1\n",
      "   0  1  0  1  0  0  1  1  0  0  0  0  1  2  0  1  0  2  0  2  0  0  0  2\n",
      "   1  2  0  0  0  1  0  1  2  1  0  0  3  2  0  0  1  0  0  0  3  0  0  0\n",
      "   0  0  1  1  4  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  2  1  2  3\n",
      "   0  1  0  1  0  0  0  0  0  0  0  1  0  0  0  0  1  0  1  0  0  0  0  0\n",
      "   1  0  0  0  0  0  0  0  2  0  1  0  0  0  0  0  0  1  1  0  0  1  0  0\n",
      "   0  5  5  0  0  0  0  0  1  1  1  1  0  2  1  1  1  0  1  1  0  3  0  0\n",
      "   0  0  0  1  1  0  0  0  2  0  0  3  0  1  0  0  0  0  1  0  0  1  0  3\n",
      "   0  0  1  0]\n",
      " [ 1  1  0  0  0  0  1  1  1  1  0  0  5  0  1  0  0  1  0  1  1  0  0  0\n",
      "   0  0  0  0  0  2  0  1  0  1  0  0  1  0  0  0  1  1  0  0  0  1  0  0\n",
      "   1  1  1  7  0  0  0  0  0  0  1  1  3  2  1  2  0  1  0  0  1  0  1  0\n",
      "   1  0  1  0  2  1  0  0  0  0  0  1  0  0  0  0  0  0  2  0  2  0  0  0\n",
      "   1  0  1  0  0  0  1  0  0  0  1  0  0  0  1  3  0  1  0  2  0  1  3  1\n",
      "   0  0  0  0  0  1  0  0  3  4  1  1  1  0  0  1  1  0  0  6  0  0  0  0\n",
      "   0  0  0  0  2  1  1  1  0  1  2  2  0  2  1  0  0  1  0  0  0  0  0  1\n",
      "   0  1  0  0  0  1  1  1  0  0  0  1  1  0  1  0  0  1  1  1  2  0  1  0\n",
      "   1  0  0  0  0  1  0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0  3  1\n",
      "   1  0  1  0  0  0  1  0  0  1  0  2  2  0  5  1  0  0  0  1  3  0  1  0\n",
      "   0  1  0  0]\n",
      " [ 0  0  0  0  2  1  0  0  0  0  1  0  4  0  0  2  1  0  1  0  0  0  1  1\n",
      "   0  1  1  0  1  0  0  0  0  0  0  1  2 16  0  3  0  0  0  0  1  0  0  0\n",
      "   0  0  0  1  2  0  0  0  1  0  0  0  0  0  0  0  1  0  0  1  0  1  1  0\n",
      "   0  0  0  0  1  0  0  0  1  1  1  1  0  1  1  0  1  0  0  0  0  1  1  0\n",
      "   0  0  0  1  1  0  0  0  0  0  0  1  0  0  0  1  0  0  1  0  0  0  1  0\n",
      "   1  1  0  0  0  0  1  1  0  0  1  0  0  1  1  0  0  2  1  0  1  0  0  0\n",
      "   1  0  1  0  1  0  0  0  1  0  0  1  1  0  1  1  0  0  0  1  1  1  2  1\n",
      "   0  1  1  1  2  0  0  0  0  1  0  0  0  1  0  1  1  0  0  0  0  1  0  1\n",
      "   0  0  0  1  1  9  1  1  0  0  0  0  0  0  0  0  0  1  0  0  1  0  1  0\n",
      "   1  1  3  1  0  1  3  1  0  0  1  5  0  0  0  0  1  1  1  0  0  0  0  0\n",
      "   2  0  0  1]]\n",
      "VECTOR FITUR\n",
      "[\"'s\", '/', '13', '8819', 'accuraci', 'achiev', 'address', 'advantag', 'advent', 'alik', 'also', 'analys', 'analysi', 'appli', 'applic', 'approach', 'appropri', 'aspect', 'attitud', 'auc', 'augment', 'author', 'automat', 'avail', 'averag', 'base', 'bay', 'becom', 'best', 'blog', 'calcul', 'case', 'caus', 'challeng', 'charact', 'chosen', 'classif', 'classifi', 'cluster', 'combin', 'compani', 'compar', 'complic', 'compon', 'comput', 'concern', 'conclud', 'consist', 'contagi', 'crm', 'custom', 'data', 'determin', 'deviat', 'differ', 'difficult', 'disambigu', 'discov', 'discuss', 'diseas', 'domain', 'done', 'drastic', 'e-health', 'effect', 'effici', 'either', 'emot', 'encompass', 'ensembl', 'evalu', 'everi', 'evolv', 'exce', 'exist', 'experi', 'express', 'extract', 'fact', 'factor', 'fast', 'feel', 'find', 'focu', 'gener', 'given', 'got', 'guid', 'handl', 'handwritten', 'health', 'height', 'help', 'hidden', 'high', 'identifi', 'ie', 'imag', 'implement', 'improv', 'inappropri', 'inconsist', 'infer', 'inform', 'interconnect', 'intersect', 'involv', 'issu', 'javanes', 'label', 'languag', 'learn', 'length', 'leverag', 'lexicon', 'like', 'line', 'lot', 'machin', 'madicin', 'main', 'major', 'make', 'mani', 'manuscript', 'market', 'markov', 'measur', 'media', 'medic', 'method', 'metric', 'mine', 'model', 'naiv', 'necessit', 'need', 'neg', 'negat', 'network', 'new', 'next', 'normal', 'object', 'offer', 'often', 'one', 'oper', 'opinion', 'organ', 'outbreak', 'outlin', 'output', 'packag', 'page', 'paper', 'parti', 'pattern', 'peopl', 'perform', 'perpendicular', 'perspect', 'plu', 'polar', 'polit', 'politician', 'posit', 'predict', 'present', 'previou', 'primari', 'principl', 'problem', 'process', 'product', 'profit', 'propos', 'public', 'question', 'r', 'reach', 'real', 'recent', 'reduc', 'refer', 'regard', 'rel', 'relev', 'research', 'result', 'rich', 'risk', 'roc', 'script', 'segment', 'select', 'sens', 'sentiment', 'sentiwordnet', 'set', 'show', 'skew', 'slant', 'slope', 'social', 'standard', 'statist', 'straighten', 'structur', 'studi', 'style', 'success', 'sureti', 'task', 'techniqu', 'term', 'text', 'three', 'thu', 'time', 'touch', 'toward', 'tweet', 'twitter', 'two', 'understand', 'unseen', 'use', 'user', 'valu', 'variou', 'via', 'view', 'vote', 'way', 'web', 'websit', 'well', 'wide', 'width', 'word', 'world', 'write', '’']\n",
      "NUMBER VECTOR FEATURES\n",
      "244\n",
      "exported all\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "REGEX = re.compile(r\"\\s\")\n",
    "def tokenize(text):\n",
    "    return [tok.strip().lower() for tok in REGEX.split(text)]\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# open all passages\n",
    "file = open(\"text1.txt\",\"r\");\n",
    "text1 = file.read()\n",
    "file = open(\"text1.txt\",\"r\");\n",
    "text2 = file.read()\n",
    "file = open(\"text3.txt\",\"r\");\n",
    "text3 = file.read()\n",
    "file = open(\"text4.txt\",\"r\");\n",
    "text4 = file.read()\n",
    "\n",
    "# dibuat huruf kecil semua\n",
    "text1=text1.lower()\n",
    "text2=text2.lower()\n",
    "text3=text3.lower()\n",
    "text4=text4.lower()\n",
    "\n",
    "# remove all punctuations\n",
    "punctuation = [\".\" , \",\" , \"(\" , \")\" , \"%\"]\n",
    "for punc in punctuation:\n",
    "\ttext1=text1.replace(punc,\"\")\n",
    "\ttext2=text2.replace(punc,\"\")\n",
    "\ttext3=text3.replace(punc,\"\")\n",
    "\ttext4=text4.replace(punc,\"\")\n",
    "\n",
    "# tokenize the article into words\n",
    "words1 = word_tokenize(text1)\n",
    "words2 = word_tokenize(text2)\n",
    "words3 = word_tokenize(text3)\n",
    "words4 = word_tokenize(text4)\n",
    "\n",
    "# filter the stopwords\n",
    "filteredText1 = [w for w in words1 if not w in stop_words]\n",
    "filteredText2 = [w for w in words2 if not w in stop_words]\n",
    "filteredText3 = [w for w in words3 if not w in stop_words]\n",
    "filteredText4 = [w for w in words4 if not w in stop_words]\n",
    "\n",
    "# stem every word\n",
    "stem1 = [ps.stem(w) for w in filteredText1];\n",
    "stem2 = [ps.stem(w) for w in filteredText2];\n",
    "stem3 = [ps.stem(w) for w in filteredText3];\n",
    "stem4 = [ps.stem(w) for w in filteredText4];\n",
    "\n",
    "# turn array to string so it could be converted to tf\n",
    "stem1 = ' '.join(stem1)\n",
    "stem2 = ' '.join(stem2)\n",
    "stem3 = ' '.join(stem3)\n",
    "stem4 = ' '.join(stem4)\n",
    "\n",
    "# put all texts into one variable\n",
    "train_set = [stem1,stem2,stem3,stem4]\n",
    "print (train_set)\n",
    "\n",
    "count_vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "data = count_vectorizer.fit_transform(train_set).toarray()\n",
    "vocab = count_vectorizer.get_feature_names()\n",
    "\n",
    "print (\"Jumlah Term FREQUENCY\")\n",
    "print (data)\n",
    "print (\"VECTOR FITUR\")\n",
    "print (vocab)\n",
    "print (\"NUMBER VECTOR FEATURES\")\n",
    "print (len(vocab))\n",
    "\n",
    "\n",
    "# export excel terms Frequency\n",
    "xls=\"Term-Frequency,\"\n",
    "for vc in vocab:\n",
    "\txls+=vc+\",\"\n",
    "\n",
    "xls += \"\\n\"\n",
    "\n",
    "# print terms Frequency for each document\n",
    "i = 0\n",
    "for row in data:\n",
    "\txls += \"Document \"+str(i)+\",\"\n",
    "\tfor col in row:\n",
    "\t\txls += str(col)+\",\"\n",
    "\txls += \"\\n\"\t\n",
    "\ti += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "xls += \"\\n\\n\\n\\n\\n\"\n",
    "# export excel Document Frequency\n",
    "xls += \"Document-Frequency,\"\n",
    "for vc in vocab:\n",
    "\txls+=vc+\",\"\n",
    "\n",
    "xls += \"\\n\"\n",
    "\n",
    "dfs = []\n",
    "for dt in data:\n",
    "\tindex2 = 0\n",
    "\txls += \",\"\n",
    "\tfor dta in dt:\n",
    "\t\tcount = 0\n",
    "\t\tif data[0][index2]>0:count += 1\n",
    "\t\tif data[1][index2]>0:count += 1\n",
    "\t\tif data[2][index2]>0:count += 1\n",
    "\t\tif data[3][index2]>0:count += 1\n",
    "\t\tdfs.append(count)\n",
    "\t\txls += str(count)+\",\"\n",
    "\t\tindex2 += 1\n",
    "\txls += \"\\n\"\n",
    "\tbreak\n",
    "\n",
    "\n",
    "xls += \"\\n\\n\\n\\n\\n\"\n",
    "# export TF-IDF\n",
    "xls += \"TF-IDF,\"\n",
    "for vc in vocab:\n",
    "\txls+=vc+\",\"\n",
    "\n",
    "xls += \"\\n\"\n",
    "\n",
    "i=0\n",
    "TFIDF=[]\n",
    "for dt in data:\n",
    "\txls += \"DOCUMENT \"+str(i)+\",\"\n",
    "\tj=0\n",
    "\tdata_tfidf=[]\n",
    "\tfor dta in dt:\n",
    "\t\t# calculate tfidf with formula tfidf = tf*log(Ndocument/df)\n",
    "\t\ttfidf = dta*math.log(4/dfs[j])\n",
    "\t\tdata_tfidf.append(tfidf)\n",
    "\t\txls += str(tfidf)+\",\"\n",
    "\t\tj += 1\n",
    "\t\n",
    "\tTFIDF.append(data_tfidf)\n",
    "\n",
    "\txls += \"\\n\"\n",
    "\ti += 1\n",
    "\t\n",
    "\n",
    "\n",
    "# calculate COSINE-SIMILARITY\n",
    "# sumber : https://janav.wordpress.com/2013/10/27/tf-idf-and-cosine-similarity/\n",
    "# penjelasan vector : http://www.mathsisfun.com/algebra/vectors-dot-product.html\n",
    "# formula cos x = document1.document2 / |document1|x|document2|\n",
    "# dot product = x1y1 + x2y2 + xiyi\n",
    "# magnitude using pytaghoras\n",
    "# magnitude = sqrt(x1^2+x2^2+xi^2) x sqrt(y1^2+y2^2+yi^2)\n",
    "\n",
    "\n",
    "xls += \"\\n\\n\\n\\n\\n\\nCOSINUS SIMILARITY,,Rumus yang digunakan = document1.document2 / |document1|x|document2|\\n\\n\\n\"\n",
    "\n",
    "xls+= \",\"\n",
    "for i in range(4):\n",
    "\txls += \"Document\"+str(i+1)+\",\"\n",
    "xls += \"\\n\"\n",
    "\n",
    "for i in range(4):\n",
    "\txls += \"Document\"+str(i+1)+\",\"\n",
    "\tfor j in range(4):\n",
    "\t\tdt1 = TFIDF[i]\n",
    "\t\tdt2 = TFIDF[j]\n",
    "\t\t# calculate dot product\n",
    "\t\tdotproduct = np.dot(dt1,dt2)\n",
    "\t\t# calculate 2 data magnitude\n",
    "\t\tmagnitude1 = np.sqrt(np.dot(dt1,dt1))\n",
    "\t\tmagnitude2 = np.sqrt(np.dot(dt2,dt2))\n",
    "\n",
    "\t\tcos_similarity = dotproduct/(magnitude1*magnitude2)\n",
    "\t\txls += str(cos_similarity)+\",\"\n",
    "\txls += \"\\n\"\n",
    "\n",
    "\n",
    "\n",
    "file = open(\"tf-idf.csv\",\"w\")\n",
    "file.write(xls)\n",
    "print (\"exported all\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# USING SKLEARN LIBRARY===========================\n",
    "\n",
    "# tfidf = TfidfVectorizer().fit_transform(train_set)\n",
    "# pairwise_similarity = tfidf * tfidf.T\n",
    "\n",
    "# print \"Jumlah Term FREQUENCY-Inverse Document Frequency=============================\"\n",
    "# print tfidf\n",
    "# print type(tfidf)\n",
    "\n",
    "# print \"Jumlah COSINE-SIMILARITY=============================\"\n",
    "# print pairwise_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
